{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, \n",
    "    desc='CIFAR-10 Dataset urlretrieve'('https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz', 'cifar-10-python.tar.gz', 'pbar.hook')\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "def _load_label_names():\n",
    "    \"\"\"\n",
    "    Load the label names from file\n",
    "    \"\"\"\n",
    "    return ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse','ship','truck']\n",
    "\n",
    "def load_cfar10_batch(cifar10_dataset_folder_path, batch_id):\n",
    "    \"\"\"\n",
    "    Load a batch of the dataset\n",
    "    \"\"\"\n",
    "    with open(cifar10_dataset_folder_path + '/data_batch_' + str(batch_id), mode= 'rb'\n",
    "    batch = pickle.load(file, encoding='latin1')\n",
    "    features = batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0\n",
    "    labels = batch['labels']\n",
    "    return features, labels\n",
    "                                                                            \n",
    "def display_stats(cifar10_dataset_folder_path, batch_id, sample_id):\n",
    "    \"\"\"\n",
    "    Display Stats of the the dataset\n",
    "    \"\"\"\n",
    "    batch_ids = list(range(1, 6))\n",
    "    if batch_id not in batch_ids:\n",
    "    print('Batch Id out of Range. Possible Batch Ids: {}'.format(batch_ids))\n",
    "    return None\n",
    "    features, labels = load_cfar10_batch(cifar10_dataset_folder_path, batch_id)\n",
    "    if not (0 <= sample_id < len(features)):\n",
    "    print('{} samples in batch {}. {} is out of range.'.format(len(features)\n",
    "    return None\n",
    "    print('\\nStats of batch {}:'.format(batch_id))\n",
    "    print('Samples: {}'.format(len(features)))\n",
    "    print('Label Counts: {}'.format(dict(zip(*np.unique(labels, return_counts=True\n",
    "    print('First 20 Labels: {}'.format(labels[:20]))\n",
    "    sample_image = features[sample_id]\n",
    "    sample_label = labels[sample_id]\n",
    "    label_names = _load_label_names()\n",
    "    print('\\nExample of Image {}:'.format(sample_id))\n",
    "    print('Image - Min Value: {} Max Value: {}'.format(sample_image.min(), sample_id))\n",
    "    print('Image - Shape: {}'.format(sample_image.shape))\n",
    "    print('Label - Label Id: {} Name: {}'.format(sample_label, label_names[sample_id]))\n",
    "    plt.axis('off')\n",
    "                                                    \n",
    "    plt.imshow(sample_image)\n",
    "                                                    \n",
    "def_preprocess_and_save(normalize, one_hot_encode, features, labels, filename):\n",
    "    \"\"\"\n",
    "    Preprocess data and save it to file\n",
    "    \"\"\"\n",
    "    features = normalize(features)\n",
    "    labels = one_hot_encode(labels)\n",
    "    pickle.dump((features, labels), open(filename, 'wb'))\n",
    "                                                \n",
    "                                        \n",
    "                                                    \n",
    "def preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode, features, labels, filename):\n",
    "    \"\"\"\n",
    "    Preprocess Training and Validation Data\n",
    "    \"\"\"\n",
    "    n_batches = 5\n",
    "    valid_features = []\n",
    "    valid_labels = []\n",
    "    for batch_i in range(1, n_batches + 1):\n",
    "    features, labels = load_cfar10_batch(cifar10_dataset_folder_path, batch_i\n",
    "    validation_count = int(len(features) * 0.1)\n",
    "    # Prprocess and save a batch of training data\n",
    "    _preprocess_and_save(\n",
    "    normalize,\n",
    "    one_hot_encode,\n",
    "    features[:-validation_count],\n",
    "    labels[:-validation_count],\n",
    "    'preprocess_batch_' + str(batch_i) + '.p')\n",
    "    # Use a portion of training batch for validation\n",
    "    valid_features.extend(features[-validation_count:])\n",
    "    valid_labels.extend(labels[-validation_count:])\n",
    "                                     \n",
    "                                     \n",
    "    # Preprocess and Save all validation data\n",
    "    _preprocess_and_save(\n",
    "    normalize,\n",
    "    one_hot_encode,\n",
    "    np.array(valid_features),\n",
    "    np.array(valid_labels),\n",
    "    'preprocess_validation.p')\n",
    "    with open(cifar10_dataset_folder_path + '/test_batch', mode='rb') as file:\n",
    "     #batch = pickle.load(file, encoding='latin1')\n",
    "    # load the training data\n",
    "    test_features = batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpo\n",
    "    test_labels = batch['labels']\n",
    "    # Preprocess and Save all training data\n",
    "    _preprocess_and_save(\n",
    "    normalize,\n",
    "    one_hot_encode,\n",
    "    np.array(test_features),\n",
    "    \n",
    "    np.array(test_labels),\n",
    "    'preprocess_training.p')\n",
    "def batch_features_labels(features, labels, batch_size):\n",
    "    \"\"\"\n",
    "    Split features and labels into batches\n",
    "    \"\"\"\n",
    "    for start in range(0, len(features), batch_size):\n",
    "    end = min(start + batch_size, len(features))\n",
    "    yield features[start:end], labels[start:end]\n",
    "def load_preprocess_training_batch(batch_id, batch_size):\n",
    "    \"\"\"\n",
    "    Load the Preprocessed Training data and return them in batches of <batch_size\n",
    "    \"\"\"\n",
    "    filename = 'preprocess_batch_' + str(batch_id) + '.p'\n",
    "    features, labels = pickle.load(open(filename, mode='rb'))\n",
    "    # Return the training data in batches of size <batch_size> or less\n",
    "    return batch_features_labels(features, labels, batch_size)\n",
    "                                     \n",
    "\n",
    "def display_image_predictions(features, labels, predictions):\n",
    "    n_classes = 10\n",
    "    label_names = _load_label_names()\n",
    "    label_binarizer = LabelBinarizer()\n",
    "    label_binarizer.fit(range(n_classes))\n",
    "    label_ids = label_binarizer.inverse_transform(np.array(labels))\n",
    "    fig, axies = plt.subplots(nrows=4, ncols=2)\n",
    "    fig.tight_layout()\n",
    "    fig.suptitle('Softmax Predictions', fontsize=20, y=1.1)\n",
    "    n_predictions = 3\n",
    "    margin = 0.05\n",
    "    ind = np.arange(n_predictions)\n",
    "    width = (1. - 2. * margin) / n_predictions\n",
    "    for image_i, (feature, label_id, pred_indicies, pred_values) in enumerate(zip\n",
    "    pred_names = [label_names[pred_i] for pred_i in pred_indicies]\n",
    "    correct_name = label_names[label_id]\n",
    "    axies[image_i][0].imshow(feature*255)\n",
    "    axies[image_i][0].set_title(correct_name)\n",
    "    axies[image_i][0].set_axis_off()\n",
    "    axies[image_i][1].barh(ind + margin, pred_values[::-1], width)\n",
    "    axies[image_i][1].set_yticks(ind + margin)\n",
    "    axies[image_i][1].set_yticklabels(pred_names[::-1])\n",
    "    axies[image_i][1].set_xticks([0, 0.5, 1.0])\n",
    "                                                                          \n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import numpy as np\n",
    "# Explore the dataset\n",
    "batch_id = 3\n",
    "sample_id = 5\n",
    "display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfTest",
   "language": "python",
   "name": "tftest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
